REDIRECT TO ANOTHER PAGE
<form action="http://192.168.5.208:8080/">
    <input type="submit" value="Call API" />
</form>
<input type="button" onclick="location.href='http://192.168.5.208:8080/test/2';" value="Go to Google" />


EXTRACT PNG, JPG TO CSV 
https://extracttable.com/

QUERY MASS UPLOAD FROM EXCEL TO POSTGRES
=CONCATENATE("INSERT INTO ""PLN_Data""(""time"",""asset_id"",""voltage_volt"",""current_ampere"",""frequency_hz"",""power_factor"",""power_kwatt"",""energy_kwh"") VALUES(","'",H2,"'",",","'",G2,"'",",","'",A2,"'",",","'",B2,"'",",","'",C2,"'",",","'",D2,"'",",","'",E2,"'",",","'",F2,"'",");")

COPY PASTE MEASUREMENT
SELECT * INTO EMS_DATA FROM EMS_2 group by *

SELECT mean("load_power") AS "load" FROM "power_readings"  WHERE $timeFilter GROUP BY time(1h) fill(0)

=REPLACE(TEXT(B7348,"YYYY-MM-DD HH:MM:SS"),18,2,"00")

SELECT ABS(SUM(current_ampere)) AS current_ampere FROM EMS_2 WHERE time >= '2021-01-01 00:00:00' and time <= '2021-01-01 00:10:00' GROUP BY asset_id
SELECT ABS(SUM(current_ampere)) AS current_ampere FROM EMS_2 WHERE time >= '2021-01-01 00:00:00' and time <= '2021-01-01 00:10:00' GROUP BY time(1m)

SELECT ABS(SUM(Global_active_power)) AS Active_power FROM Data WHERE $timeFilter GROUP BY time(1h)
SELECT ABS(SUM(Global_reactive_power)) AS Reactive_power FROM Data WHERE $timeFilter GROUP BY time(1h)

SELECT mean("value") FROM "cpu" WHERE $timeFilter GROUP BY time($__interval) fill(null) ORDER BY time DESC
SELECT mean("value") FROM "cpu" WHERE time >= now() - 30m GROUP BY time(10s) fill(null) ORDER BY time DESC
Select time, Global_active_power from Data  where $timeFilter order by asc

SELECT ABS(SUM(Global_active_power)) as Global_active_power FROM Data WHERE time >= '2006-12-16 17:24:00' and time <= '2006-12-16 17:28:00'GROUP BY time(1m)
SELECT ABS(SUM(Global_active_power)) as Global_active_power FROM Data WHERE time >= '2006-12-16 17:24:00' and time <= '2006-12-16 17:28:00'GROUP BY time(1m)

Select time, Global_active_power from Data  where time >= '2006-12-16 17:24:00' and time <= '2006-12-16 17:28:00' order by asc

show field keys from Asset_Data
to_date(to_char("Date_time", 'YYYY-MM-DD HH24:MI:SS'), 'YYYY-MM-DD HH24:MI:SS') AS time,
influxd run -config E:\InfluxDB\influxdb-1.8.4-1\influxdb.conf

ACCESS SELECT * FROM WITH ANOTHER COMPUTER IN INFLUXDB
curl -G "http://192.168.5.216:8086/query?db=Grafana" --data-urlencode "q=SELECT * FROM "EMS""
curl -XPOST "http://192.168.5.216:8086/query?db=Grafana" --data-urlencode "q=DROP MEASUREMENT "EMS""

EXPORT FROM DATABASE TO CSV AND VISE VERSA TO INFLUXDB
go get -v github.com/jpillora/csv-to-influxdb
csv-to-influxdb -s http://192.168.5.216:8086 -d Grafana -m Asset_Data -t Id --timestamp-column Date_time -b 9999999 -h 99999999 -n -f D:\Work\PLN\demo.csv
csv-to-influxdb -s http://192.168.5.216:8086 -d Grafana -m Asset_Data2 -t asset_id,no --timestamp-column Time -b 9999999 -h 99999999 -n -f D:\Work\PLN\PLN_January_2021.csv

EXPORT FROM DATABASE TO CSV AND VISE VERSA TO POSTGRESQL
\copy "household_poer_consumption" TO 'D:\Work\PLN\house_test.csv' DELIMITER ',' CSV HEADER; (EXPORT)
\copy "EMS_HIERARCHY"(asset_id,parent_id,asset_name,description,enabled) FROM 'D:\Work\PLN\EMS_HIERARCHY.csv' DELIMITER ',' CSV HEADER;(IMPORT)
pg_dump -U postgres -h 192.168.5.216 -p 5432 --table=household_poer_consumption grafana > > dumpFile.sql

RUN SQL QUERY IN REMOTE SERVER
psql -h 192.168.5.216 -U postgres -d grafana -a -f D:\Work\PLN\PLN_January_2021

ENV AT GOLANG TO CONNECT W POSTGRES
DBHOST="postgres://postgres:1m4dm1n@192.168.5.216:5432/grafana?sslmode=disable"

CONNECT TO POSTGRES, AT MIDDLEWARE (HANDLERS.GO)
// Open the connection
	db, err := sql.Open("postgres", os.Getenv("DBHOST"))

MAKE A TABLE IN POSTGRES
CREATE TABLE users (
    userid SERIAL PRIMARY KEY,
    name TEXT,
    age INT,
    location TEXT
);

FOR CHECKING ENV IN VS CODE
GO TO CONSOLE IN VS CODE AND RUN  process.env['PATH']

Set-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy Unrestricted

TO LOGIN POSTGRES IN DIFFERENT COMP
psql -h 192.168.5.216 -p 5432 -d grafana -U postgres -W

Make a grafana plugin
go to data/plugins

